{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78dcfeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import sqlite3\n",
    "from sqlite3 import Error\n",
    "\n",
    "def show_tables(conn):\n",
    "    \"\"\" show all tables in the database \"\"\"\n",
    "    cur = conn.cursor()\n",
    "    query = \"SELECT name FROM sqlite_master WHERE type='table';\"\n",
    "    cur.execute(query)\n",
    "    result = cur.fetchall()\n",
    "    return [result[i][0] for i in range(len(result))]\n",
    "\n",
    "def drop_temporary_tables(conn):\n",
    "    \"\"\" drop temporary tables in the database \"\"\"\n",
    "    cur = conn.cursor()\n",
    "    temporary_table_names = [table for table in show_tables(conn) if 'temporary' in table]\n",
    "    for temporary_table_name in temporary_table_names:\n",
    "        cur.execute(f\"DROP TABLE IF EXISTS {temporary_table_name};\")\n",
    "    conn.commit()\n",
    "    \n",
    "def print_query(query,conn):\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(query)\n",
    "    result = cur.fetchall()\n",
    "    for row in result:\n",
    "        print(row)\n",
    "\n",
    "def get_recording_start_stop(conn):\n",
    "    \"\"\"\n",
    "    author: Andrew Smith\n",
    "    date: 17 April 2025\n",
    "    description: Get recording start and stop times from the zdb database \n",
    "    table internal_property. The table internal_property gets created the \n",
    "    first time someone opens a\n",
    "    Ponemah experiment in NeuroScore. So, the recording start stop times are\n",
    "    somewhere in the Ponemah files.\n",
    "\n",
    "    future: get recording start stop directly from Ponemah files to try and\n",
    "    avoid zdb\n",
    "    \"\"\"\n",
    "    \"\"\" get recording start and stop times \"\"\"\n",
    "    cur = conn.cursor()\n",
    "    query = \"SELECT value FROM internal_property WHERE key='RecordingStart'\"\n",
    "    cur.execute(query)\n",
    "    result = cur.fetchall()\n",
    "    recording_start = int(result[0][0])\n",
    "    query = \"SELECT value FROM internal_property WHERE key='RecordingStop'\"\n",
    "    cur.execute(query)\n",
    "    result = cur.fetchall()\n",
    "    recording_stop = int(result[0][0])\n",
    "    return recording_start, recording_stop\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.functional import relu\n",
    "from mne.io import read_raw_edf\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self,in_feature_maps,out_feature_maps,n_features) -> None:\n",
    "        super().__init__()\n",
    "        self.c1 = nn.Conv1d(in_feature_maps,out_feature_maps,kernel_size=8,padding='same',bias=False)\n",
    "        self.bn1 = nn.LayerNorm((out_feature_maps,n_features),elementwise_affine=False)\n",
    "\n",
    "        self.c2 = nn.Conv1d(out_feature_maps,out_feature_maps,kernel_size=5,padding='same',bias=False)\n",
    "        self.bn2 = nn.LayerNorm((out_feature_maps,n_features),elementwise_affine=False)\n",
    "\n",
    "        self.c3 = nn.Conv1d(out_feature_maps,out_feature_maps,kernel_size=3,padding='same',bias=False)\n",
    "        self.bn3 = nn.LayerNorm((out_feature_maps,n_features),elementwise_affine=False)\n",
    "\n",
    "        self.c4 = nn.Conv1d(in_feature_maps,out_feature_maps,1,padding='same',bias=False)\n",
    "        self.bn4 = nn.LayerNorm((out_feature_maps,n_features),elementwise_affine=False)\n",
    "\n",
    "    def forward(self,x):\n",
    "        identity = x\n",
    "        x = self.c1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = relu(x)\n",
    "\n",
    "        x = self.c2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = relu(x)\n",
    "\n",
    "        x = self.c3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = relu(x)\n",
    "\n",
    "        identity = self.c4(identity)\n",
    "        identity = self.bn4(identity)\n",
    "\n",
    "        x = x+identity\n",
    "        x = relu(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class Frodo(nn.Module):\n",
    "    def __init__(self,n_features) -> None:\n",
    "        super().__init__()\n",
    "        self.n_features = n_features\n",
    "        self.block1 = ResidualBlock(1,8,n_features)\n",
    "        self.block2 = ResidualBlock(8,16,n_features)\n",
    "        self.block3 = ResidualBlock(16,16,n_features)\n",
    "\n",
    "        self.gap = nn.AvgPool1d(kernel_size=n_features)\n",
    "        self.fc1 = nn.Linear(in_features=16,out_features=3)\n",
    "    def forward(self,x,classification=True):\n",
    "        x = x.view(-1,1,self.n_features)\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.gap(x)\n",
    "        if(classification):\n",
    "            x = self.fc1(x.squeeze())\n",
    "            return x\n",
    "        else:\n",
    "            return x.squeeze()\n",
    "        \n",
    "class Gandalf(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = Frodo(n_features=5000)\n",
    "        self.lstm = nn.LSTM(16,32,bidirectional=True)\n",
    "        self.fc1 = nn.Linear(64,3)\n",
    "    def forward(self,x_2d,classification=True):\n",
    "        x_2d = x_2d.view(-1,9,1,5000)\n",
    "        x = []\n",
    "        for t in range(x_2d.size(1)):\n",
    "            xi = self.encoder(x_2d[:,t,:,:],classification=False)\n",
    "            x.append(xi.unsqueeze(0))\n",
    "        x = torch.cat(x)\n",
    "        out,_ = self.lstm(x)\n",
    "        if(classification):\n",
    "            x = self.fc1(out[-1])\n",
    "        else:\n",
    "            x = out[-1]\n",
    "        return x\n",
    "    \n",
    "class EEGDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,X):\n",
    "        self.len = len(X)\n",
    "        self.X = torch.cat([torch.zeros(4,5000),X,torch.zeros(4,5000)])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx:idx+9].flatten()\n",
    "    \n",
    "def score_recording(edf_path,model_path,eeg_ch_name,device,zdb_path):\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileExistsError(model_path)\n",
    "\n",
    "    model = Gandalf()\n",
    "    model.load_state_dict(torch.load(model_path,map_location='cpu'))\n",
    "\n",
    "    raw = read_raw_edf(input_fname=edf_path)\n",
    "    data = raw.get_data(picks=eeg_ch_name)\n",
    "    eeg = torch.from_numpy(data[0]).float()\n",
    "    eeg = eeg.view(-1,5000)\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        dataloader = DataLoader(EEGDataset(eeg),batch_size=32)\n",
    "        logits = torch.cat([model(Xi.to(device)).cpu() for Xi in tqdm(dataloader)])\n",
    "        y_pred = logits.softmax(dim=1).argmax(axis=1)\n",
    "\n",
    "    pd.DataFrame(y_pred,columns=['y_pred']).to_csv(f\"{zdb_path.replace('.zdb','.y_pred')}\",index=False)\n",
    "    # pd.DataFrame(logits).to_csv(f\"{edf_path.replace('.edf','.logits')}\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c31e60b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from /home/andrew/aurora-sleep-staging/data/25-Feb-AAV-2.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_593868/3555921427.py:162: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path,map_location='cpu'))\n",
      "  0%|          | 0/1899 [00:00<?, ?it/s]/home/andrew/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:370: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:1036.)\n",
      "  return F.conv1d(\n",
      "100%|██████████| 1899/1899 [01:15<00:00, 25.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from /home/andrew/aurora-sleep-staging/data/25-Feb-AAV-3.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1899/1899 [01:12<00:00, 26.10it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "recordings_dir = f'data'\n",
    "recording_ids = sorted([file.replace('.zdb','') for file in os.listdir(recordings_dir) if file.endswith('.zdb')])\n",
    "eeg_ch_name = \"EEG 1\"\n",
    "device = 'cuda'\n",
    "model_path = 'gandalf.pt'\n",
    "\n",
    "for recording_id in recording_ids:\n",
    "    zdb_path = f'{recordings_dir}/{recording_id}.zdb'\n",
    "    edf_path = f'{recordings_dir}/{recording_id.split(\".\")[1]}.edf'\n",
    "    score_recording(edf_path=edf_path,model_path=model_path,eeg_ch_name=eeg_ch_name,device=device,zdb_path=zdb_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e1f95409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration: 168.0 hours 44.0 minutes 52.0 seconds\n",
      "duration seconds: 607492.0\n",
      "recording start: 638760838920000000\n",
      "recording stop: 638766913840000000\n",
      "['internal_property', 'logging_log_entry', 'workspace_workspace', 'scoring_revision', 'scoring_marker', 'scoring_key', 'scoring_revision_to_key']\n",
      "['internal_property', 'logging_log_entry', 'workspace_workspace', 'scoring_revision', 'scoring_marker', 'scoring_key', 'scoring_revision_to_key']\n",
      "duration: 168.0 hours 44.0 minutes 52.0 seconds\n",
      "duration seconds: 607492.0\n",
      "recording start: 638760838920000000\n",
      "recording stop: 638766913840000000\n",
      "['internal_property', 'logging_log_entry', 'workspace_workspace']\n",
      "['internal_property', 'logging_log_entry', 'workspace_workspace', 'scoring_revision', 'scoring_marker', 'scoring_key', 'scoring_revision_to_key']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "recordings_dir = f'data'\n",
    "recording_ids = sorted([file.replace('.zdb','') for file in os.listdir(recordings_dir) if file.endswith('.zdb')])\n",
    "eeg_ch_name = \"EEG 1\"\n",
    "device = 'cuda'\n",
    "model_path = 'gandalf.pt'\n",
    "\n",
    "for recording_id in recording_ids:\n",
    "    zdb_path = f'{recordings_dir}/{recording_id}.zdb'\n",
    "    y_pred = pd.read_csv(f\"{zdb_path.replace('.zdb','.y_pred')}\")\n",
    "    y_pred = pd.DataFrame(y_pred).astype('str')\n",
    "    y_pred.loc[y_pred['y_pred'] == '0','y_pred'] = 'Sleep-Paradoxical'\n",
    "    y_pred.loc[y_pred['y_pred'] == '1','y_pred'] = 'Sleep-SWS'\n",
    "    y_pred.loc[y_pred['y_pred'] == '2','y_pred'] = 'Sleep-Wake'\n",
    "\n",
    "    try:\n",
    "        conn = sqlite3.connect(zdb_path)\n",
    "    except Error as e:\n",
    "        print(e)\n",
    "    \n",
    "    recording_start,recording_stop = get_recording_start_stop(conn)\n",
    "    length_ns = recording_stop - recording_start # ns\n",
    "    length_s = length_ns * 1e-7 # s\n",
    "    hh = length_s // 3600\n",
    "    mm = (length_s % 3600) // 60\n",
    "    ss = ((length_s % 3600) % 60)\n",
    "\n",
    "    print(\"duration:\",hh,\"hours\",mm,\"minutes\",ss,\"seconds\")\n",
    "    print(\"duration seconds:\",length_s)\n",
    "    print(\"recording start:\",recording_start)\n",
    "    print(\"recording stop:\",recording_stop)\n",
    "\n",
    "    tables = show_tables(conn)\n",
    "\n",
    "    print(tables)\n",
    "\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    if 'scoring_revision' not in tables:\n",
    "        query = \"CREATE TABLE scoring_revision (id INTEGER PRIMARY KEY, name TEXT, is_deleted INTEGER(1), tags TEXT, version INTEGER(8), owner TEXT, date_created INTEGER(8));\"\n",
    "        cur.execute(query)\n",
    "\n",
    "    if 'scoring_marker' not in tables:\n",
    "        query = \"CREATE TABLE scoring_marker (id INTEGER PRIMARY KEY, starts_at INTEGER(8), ends_at INTEGER(8), notes TEXT, type TEXT, location TEXT, is_deleted INTEGER(1), key_id INTEGER);\"\n",
    "        cur.execute(query)\n",
    "\n",
    "    if 'scoring_key' not in tables:\n",
    "        query = \"CREATE TABLE scoring_key (id INTEGER PRIMARY KEY, date_created INTEGER(8), name TEXT, owner TEXT, type TEXT);\"\n",
    "        cur.execute(query)\n",
    "\n",
    "    if 'scoring_revision_to_key' not in tables:\n",
    "        query = \"CREATE TABLE scoring_revision_to_key (revision_id INTEGER(8), key_id INTEGER(8));\"\n",
    "        cur.execute(query)\n",
    "\n",
    "    conn.commit()\n",
    "\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"INSERT INTO scoring_key (type) VALUES ('Automatic');\")\n",
    "    conn.commit()\n",
    "    key_id = cur.lastrowid\n",
    "    \n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"INSERT INTO scoring_revision (name, is_deleted, version) VALUES ('Gandalf', 0, 0);\")\n",
    "    conn.commit()\n",
    "    scoring_revision_id = cur.lastrowid\n",
    "\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(f\"INSERT INTO scoring_revision_to_key (revision_id, key_id) VALUES ({scoring_revision_id}, {key_id});\")\n",
    "    conn.commit()\n",
    "\n",
    "    tables = show_tables(conn)\n",
    "\n",
    "    print(tables)\n",
    "\n",
    "    epoch_start_time = int(int(recording_start * 1e-8) * 1e8) # decaseconds\n",
    "    epoch_stop_time = epoch_start_time + int(1e8)\n",
    "\n",
    "    for score in y_pred.values.flatten():\n",
    "        query = f\"\"\"\n",
    "            INSERT INTO scoring_marker \n",
    "            (starts_at, ends_at, notes, type, location, is_deleted, key_id)\n",
    "            VALUES \n",
    "            ({epoch_start_time}, {epoch_stop_time}, '', '{score}', '', 0, {key_id});\n",
    "            \"\"\"\n",
    "        \n",
    "        epoch_start_time = epoch_stop_time\n",
    "        epoch_stop_time = epoch_start_time + int(1e8)\n",
    "        \n",
    "        cur = conn.cursor()\n",
    "        cur.execute(query)\n",
    "        \n",
    "    conn.commit()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "057906c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/25-Feb-AAV.25-Feb-AAV-2.20250225173812.zdb\n",
      "data/25-Feb-AAV.25-Feb-AAV-3.20250225173812.zdb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(f'tar -czvf data.tar.gz data/*.zdb')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
